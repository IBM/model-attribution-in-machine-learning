Background: Clinical guidelines on smoking cessation contain recommendations for various evidence-based methods. The goal of this study was to provide a represen- tative analysis for Germany of the percentage of smokers who try to quit smoking at least once per year, the use of evidence-based methods and other methods of smoking cessation, and potential associations of the use of such methods with the degree of tobacco dependence and with socioeconomic features
Methods: Data from 19 waves of the German Smoking Behavior Questionnaire (Deutsche Befragung zum Rauchverhalten, DEBRA), from the time period June/July 2016 to June/July 2019, were analyzed. Current smokers and recent ex-smokers (<12 months without smoking) were asked about their smoking cessation attempts in the past year and the methods they used during the last attempt (naming more than one method was permitted). The degree of tobacco dependence in current smokers was assessed with the Heaviness of Smoking Index.
Results: Out of 11 109 current smokers and 407 recent ex-smokers, 19.9% (95% confidence interval: [19.1; 20.6]) had tried to quit smoking at least once in the preceding year. 13.0% of them [11.6; 14.5] had used at least one evidence-based method during their last attempt. The stronger the tobacco dependence, the more likely the use of an evidence-based method (odds ratio [OR] = 1.27 [1.16; 1.40]). Pharmacotherapy (nicotine replacement therapy, medication) was used more com- monly by persons with higher incomes (OR = 1.44 per 1000 euro/month [1.28; 1.62]). Electronic cigarettes were the most commonly used single type of smoking cessation support (10.2 % [9.0; 11.6]).
Conclusion: In Germany, only one in five smokers tries to quit smoking at least once per year. Such attempts are only rarely supported by evidence-based methods and are thus likely to fail. The high cost of treatment must be borne by the individual and thus fall disproportionately on poorer smokers. It follows that there is an urgent need for vered by health insurance pro- viders, in order to give all smokers fair and equal access to the medical care they need.
Healthy aging is the ability to maintain independence, purpose, vitality, and quality of life into old age despite unexpected medical conditions, accidents, and unhelpful social determinants of health. Exercise, or physical activity, is an important component of healthy aging, preventing or mitigating falls, pain, sarcopenia, osteoporosis, and cognitive impairment. A well-balanced exercise program includes daily aerobic, strength, balance, and flexibility components. Most older adults do not meet the currently recommended minutes of regular physical activity weekly. Counseling by health care providers may help older adults improve exercise habits, but it is also important to take advantage of community-based exercise opportunities.
Introduction: Depression is a very prevalent mental disorder affecting 340 million people globally and is projected to become the leading cause of disability and the second leading contributor to the global burden of disease by the year 2020.
Aim: In this paper, we review the evidence published to date in order to determine whether exercise and physical activity can be used as therapeutic means for acute and chronic depression. Topics covered include the definition, classification criteria and treatment of depression, the link between β-endorphin and exercise, the efficacy of exercise and physical activity as treatments for depression, properties of exercise stimuli used in intervention programs, as well as the efficacy of exercise and physical activity for treating depression in diseased individuals.
Conclusions: The presented evidence suggests that exercise and physical activity have beneficial effects on depression symptoms that are comparable to those of antidepressant treatments.
Rough graph is the graphical structure of information system with imprecise knowledge. Tong He designed the properties of rough graph in 2006[6] and following that He and Shi introduced the notion of edge rough graph[7]. He et al developed the concept of weighted rough graph with weighted attributes[6]. In this paper, we introduce a new type of labeling called Even vertex {\zeta}- graceful labeling as weight value for edges. We investigate this labeling for some special graphs like rough path graph, rough cycle graph, rough comb graph, rough ladder graph and rough star graph.
The currently dominating artificial intelligence and machine learning technology, neural networks, builds on inductive statistical learning. Neural networks of today are information processing systems void of understanding and reasoning capabilities, consequently, they cannot explain promoted decisions in a humanly valid form. In this work, we revisit and use fundamental philosophy of science theories as an analytical lens with the goal of revealing, what can be expected, and more importantly, not expected, from methods that aim to explain decisions promoted by a neural network. By conducting a case study we investigate a selection of explainability method's performance over two mundane domains, animals and headgear. Through our study, we lay bare that the usefulness of these methods relies on human domain knowledge and our ability to understand, generalise and reason. The explainability methods can be useful when the goal is to gain further insights into a trained neural network's strengths and weaknesses. If our aim instead is to use these explainability methods to promote actionable decisions or build trust in ML-models they need to be less ambiguous than they are today. In this work, we conclude from our study, that benchmarking explainability methods, is a central quest towards trustworthy artificial intelligence and machine learning.
A galaxy's morphological features encode details about its gas content, star formation history, and feedback processes, which play important roles in regulating its growth and evolution. We use deep convolutional neural networks (CNNs) to learn a galaxy's optical morphological information in order to estimate its neutral atomic hydrogen (HI) content directly from SDSS gri image cutouts. We are able to accurately predict a galaxy's logarithmic HI mass fraction, ≡log(MHI/M⋆), by training a CNN on galaxies in the ALFALFA 40% sample. Using pattern recognition (PR), we remove galaxies with unreliable  estimates. We test CNN predictions on the ALFALFA 100%, xGASS, and NIBLES catalogs, and find that the CNN consistently outperforms previous estimators. The HI-morphology connection learned by the CNN appears to be constant in low- to intermediate-density galaxy environments, but it breaks down in the highest-density environments. We also use a visualization algorithm, Gradient-weighted Class Activation Maps (Grad-CAM), to determine which morphological features are associated with low or high gas content. These results demonstrate that CNNs are powerful tools for understanding the connections between optical morphology and other properties, as well as for probing other variables, in a quantitative and interpretable manner.
With the first observations of debris disks as well as proposed planets around white dwarfs, the question of how rocky planets around such stellar remnants can be characterized and probed for signs of life becomes tangible. White dwarfs are similar in size to Earth and have relatively stable environments for billions of years after initial cooling, making them intriguing targets for exoplanet searches and terrestrial planet atmospheric characterization. Their small size and the resulting large planet transit signal allows observations with next generation telescopes to probe the atmosphere of such rocky planets, if they exist. We model high-resolution transmission spectra for planets orbiting white dwarfs from as they cool from 6,000-4,000 K, for i) planets receiving equivalent irradiation to modern Earth, and ii) planets orbiting at the distance around a cooling white dwarf which allows for the longest continuous time in the habitable zone. All high-resolution transmission spectra will be publicly available online upon publication of this study and can be used as a tool to prepare and interpret upcoming observations with JWST, the Extremely Large Telescopes as well as mission concepts like Origins, HabEx, and LUVOIR.
One of the main mechanisms that could drive mass outflows on parsec scales in AGN is thermal driving. The same X-rays that ionize and heat the plasma are also expected to make it thermally unstable. Indeed, it has been proposed that the observed clumpiness in AGN winds is caused by thermal instability (TI). While many studies employing time-dependent numerical simulations of AGN outflows have included the necessary physics for TI, none have so far managed to produce clumpiness. Here we present the first such clumpy wind simulations in 1-D and 2-D, obtained by simulating parsec scale outflows irradiated by an AGN. By combining an analysis of our extensive parameter survey with physical arguments, we show that the lack of clumps in previous numerical models can be attributed to the following three effects: (i) insufficient radiative heating or other physical processes that prevent the outflowing gas from entering the TI zone; (ii) the stabilizing effect of stretching (due to rapid radial acceleration) in cases where the gas enters the TI zone; and (iii) a flow speed effect: in circumstances where stretching is inefficient, the flow can still be so fast that it passes through the TI zone too quickly for perturbations to grow. Besides these considerations, we also find that a necessary condition to trigger TI in an outflow is for the pressure ionization parameter to decrease along a streamline once gas enters a TI zone.
Since the outbreak of COVID-19, it has rapidly evolved into a sudden and major public health emergency globally. With the variants of COVID-19, the difficulty of pandemic control continues to increase, which has brought significant costs to the society. The existing pandemic control zoning method ignores the impact on residents'lives. In this study, we propose a refined and low-cost pandemic control method by scientifically delineating zoning areas. First, a spatial interaction network is built up based on the multimodal transport travel data in Nanjing, China, and an improved Leiden community detection method based on the gravity model is used to obtain a preliminary zoning scheme. Then, we use spatial constraints to correct the results with the discrete spatial distribution. Finally, reasonable zones for pandemic control are obtained. The modularity of the algorithm results is 0.4185, proving that the proposed method is suitable for pandemic control zoning. The proposed method is also demonstrated to be able to minimize traffic flows between pandemic control areas and only 24.8% of travel connections are cut off, thus reducing the impact of pandemic control on residents'daily life and reducing the cost of pandemic control. The findings can help to inform sustainable strategies and suggestions for the pandemic control.
Evaluating the treatment effects has become an important topic for many applications. However, most existing literature focuses mainly on the average treatment effects. When the individual effects are heavy-tailed or have outlier values, not only may the average effect not be appropriate for summarizing the treatment effects, but also the conventional inference for it can be sensitive and possibly invalid due to poor large-sample approximations. In this paper we focus on quantiles of individual effects, which can be more robust measures of treatment effects in the presence of extreme individual effects. Moreover, our inference for quantiles of individual effects are purely randomization-based, which avoids any distributional assumption on the units. We first consider inference for stratified randomized experiments, extending the recent work of Caughey et al. (2021). The calculation of valid p-values for testing null hypotheses on quantiles of individual effects involves linear integer programming, which is generally NP hard. To overcome this issue, we propose a greedy algorithm with a certain optimal transformation, which has much lower computational cost, still leads to valid p-values and is less conservative than the usual relaxation by dropping the integer constraint. We then extend our approach to matched observational studies and propose sensitivity analysis to investigate to what extent our inference on quantiles of individual effects is robust to unmeasured confounding. Both the randomization inference and sensitivity analysis are simultaneously valid for all quantiles of individual effects, which are actually free lunches added to the conventional analysis assuming constant effects. Furthermore, the inference results can be easily visualized and interpreted.
cpop: Detecting changes in piecewise-linear signals. Changepoint detection is an important problem with applications across many application domains. There are many different types of changes that one may wish to detect, and a wide-range of algorithms and software for detecting them. However there are relatively few approaches for detecting changes-in-slope in the mean of a signal plus noise model. We describe the R package, cpop, available on the Comprehensive R Archive Network (CRAN). This package implements CPOP, a dynamic programming algorithm, to find the optimal set of changes that minimises an L_0 penalised cost, with the cost being a weighted residual sum of squares. The package has extended the CPOP algorithm so it can analyse data that is unevenly spaced, allow for heterogeneous noise variance, and allows for a grid of potential change locations to be different from the locations of the data points. There is also an implementation that uses the CROPS algorithm to detect all segmentations that are optimal as you vary the L_0 penalty for adding a change across a continuous range of values.
Contrastive Audio-Language Learning for Music. As one of the most intuitive interfaces known to humans, natural language has the potential to mediate many tasks that involve human-computer interaction, especially in application-focused fields like Music Information Retrieval. In this work, we explore cross-modal learning in an attempt to bridge audio and language in the music domain. To this end, we propose MusCALL, a framework for Music Contrastive Audio-Language Learning. Our approach consists of a dual-encoder architecture that learns the alignment between pairs of music audio and descriptive sentences, producing multimodal embeddings that can be used for text-to-audio and audio-to-text retrieval out-of-the-box. Thanks to this property, MusCALL can be transferred to virtually any task that can be cast as text-based retrieval. Our experiments show that our method performs significantly better than the baselines at retrieving audio that matches a textual description and, conversely, text that matches an audio query. We also demonstrate that the multimodal alignment capability of our model can be successfully extended to the zero-shot transfer scenario for genre classification and auto-tagging on two public datasets. Digitalisation influences all parts of society. In its heart resides Artificial Intelligence (AI) and Machine Learning (ML), technologies with roots in natural science and a third-person objectivising stance Grimm (2016). Research in the area then inherits values that are concerned with ground truth, optimising class probability, minimising bias in training data and mitigating consequences of data drift.
This paper focus on inductive statistical learning approaches used in the currently dominating ML technology, neural networks. This is a technology that can learn from raw input data in the form of images, sound or text Lecun et al. (2015).
In the study, we find that the XAI-methods delivers vague and imprecise evidence and this leaves interpretation and judging the evidence presented to human abilities. Notwithstanding this, we believe that the tested XAI-methods, especially when used in an ensemble, can deliver valuable insights on strengths and weaknesses related to the ML-model’s cap- abilities.
In the background section that follows, we present theories related to scientific explanations, inductive statistical learning and concepts. This is followed by a section on the methodological approach leading to our study results. The article ends with a discussion section that contextualises our conclusions. We then end the article by concluding the results.
